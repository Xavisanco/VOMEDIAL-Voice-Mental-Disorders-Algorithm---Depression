{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f7069b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORT GENERAL LIBRARIES\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "from scipy.stats import ttest_1samp, ttest_ind, zscore, skew, kurtosis\n",
    "from scipy import stats as st\n",
    "from matplotlib import ticker\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import random\n",
    "from matplotlib.ticker import ScalarFormatter, FormatStrFormatter\n",
    "\n",
    "### IMPORT LIBRARIES FOR MODELING\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, make_scorer, f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ce17ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gaussian_kernel(x, y, sigma):\n",
    "    min_length = min(len(x), len(y))\n",
    "    x = x[:min_length]\n",
    "    y = y[:min_length]\n",
    "    return np.exp(-np.sum((x - y)**2) / (2 * sigma**2))\n",
    "\n",
    "def compare_individual_imfs(df1, df2, sigma=.5, threshold=1e-10):\n",
    "    results = []\n",
    "    significant_indices = []\n",
    "    for i in range(16):\n",
    "        imf1 = df1.iloc[:, i].values\n",
    "        imf2 = df2.iloc[:, i].values\n",
    "        similarities = [gaussian_kernel(x, y, sigma) for x, y in zip(imf1, imf2)]\n",
    "        avg_similarity = np.mean(similarities)\n",
    "        results.append(avg_similarity)\n",
    "        if avg_similarity <= threshold:\n",
    "            significant_indices.append(i+1)\n",
    "    return results, significant_indices\n",
    "\n",
    "df1 = female_dep_imfs # see utils.ipynb field in src folder\n",
    "df2 = female_health_imfs\n",
    "\n",
    "similarities, significant_imfs = compare_individual_imfs(df1, df2)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def extract_statistics(imfs_array, window_size=10000):\n",
    "    n_signals, n_imfs, n_samples = imfs_array.shape\n",
    "    n_windows = n_samples // window_size\n",
    "    if n_windows == 0 or n_imfs == 0:\n",
    "        raise ValueError(\"There are not enough samples or IMFs to form a single window.\")\n",
    "    all_stats = np.zeros((n_signals, n_imfs * 5))\n",
    "    for i in range(n_signals):\n",
    "        signal_stats = []\n",
    "        for j in range(n_imfs):\n",
    "            signal = imfs_array[i, j]\n",
    "            stats = []\n",
    "            for w in range(n_windows):\n",
    "                window = signal[w*window_size:(w+1)*window_size]\n",
    "                stats.append([\n",
    "                    skew(window),\n",
    "                    kurtosis(window),\n",
    "                    np.median(window),\n",
    "                    np.std(window),\n",
    "                    np.mean(window)\n",
    "                ])\n",
    "            stats = np.array(stats)\n",
    "            signal_stats.extend(stats.mean(axis=0))\n",
    "        all_stats[i, :] = signal_stats\n",
    "    return all_stats\n",
    "\n",
    "# Función para aplicar el balanceo de datos\n",
    "def balance_data(X, y):\n",
    "    class_0 = np.where(y == 0)[0]\n",
    "    class_1 = np.where(y == 1)[0]\n",
    "\n",
    "    # Determinamos el número mínimo de filas entre las dos clases\n",
    "    min_size = min(len(class_0), len(class_1))\n",
    "\n",
    "    # Seleccionamos aleatoriamente las filas para balancear las clases\n",
    "    balanced_indices_0 = np.random.choice(class_0, min_size, replace=False)\n",
    "    balanced_indices_1 = np.random.choice(class_1, min_size, replace=False)\n",
    "\n",
    "    balanced_indices = np.concatenate([balanced_indices_0, balanced_indices_1])\n",
    "\n",
    "    # Balanceamos X e y\n",
    "    X_balanced = X[balanced_indices]\n",
    "    y_balanced = y[balanced_indices]\n",
    "\n",
    "    return X_balanced, y_balanced\n",
    "\n",
    "imfs_array = np.stack([np.stack([df1.iloc[i, idx] for idx in significant_imfs]) for i in range(df1.shape[0])])\n",
    "print(imfs_array.shape)  # Debe ser (n_signals, n_imfs, n_samples)\n",
    "\n",
    "imfs_array2 = np.stack([np.stack([df2.iloc[i, idx] for idx in significant_imfs]) for i in range(df2.shape[0])])\n",
    "print(imfs_array2.shape)\n",
    "\n",
    "df1_stats = extract_statistics(imfs_array)\n",
    "df2_stats = extract_statistics(imfs_array2)\n",
    "\n",
    "X_stats = np.vstack([df1_stats, df2_stats])\n",
    "y_labels = np.concatenate([np.ones(df1_stats.shape[0]), np.zeros(df2_stats.shape[0])])  \n",
    "\n",
    "X_balanced, y_balanced = balance_data(X_stats, y_labels)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=42)\n",
    "\n",
    "models = {\n",
    "    'Gradient Boosting': (GradientBoostingClassifier(random_state=42), {\n",
    "        'n_estimators': list(range(1, 35, 5)),\n",
    "        'learning_rate': [0.1, 0.2, 0.3],\n",
    "        'max_depth': [1, 2, 3, 5]\n",
    "    })\n",
    "}\n",
    "\n",
    "def plot_confusion_matrix(cm, class_names, model_name):\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "results = []\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "for model_name, (model, param_grid) in models.items():\n",
    "    print(f\"Evaluando {model_name}...\")\n",
    "\n",
    "    param_grid_prefixed = {f'model__{key}': value for key, value in param_grid.items()}\n",
    "\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    pipeline = Pipeline([('scaler', StandardScaler()), ('model', model)])\n",
    "    grid_search = GridSearchCV(pipeline, param_grid_prefixed, cv=5, scoring='f1', n_jobs=-1, verbose=0)\n",
    "    \n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "    best_scores = grid_search.cv_results_\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test_scaled)\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    TN, FP, FN, TP = cm.ravel() if cm.size == 4 else (cm[0,0], cm[0,1], cm[1,0], cm[1,1])\n",
    "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "\n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'Best Model': best_model,\n",
    "        'Parameters': grid_search.best_params_,\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'F1 Score': f1_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'Specificity': specificity\n",
    "    })\n",
    "\n",
    "    plot_confusion_matrix(cm, class_names=['Healthy', 'Depressed'], model_name=model_name)\n",
    "\n",
    "def plot_roc_curves(results, X_test_scaled, y_test):\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    \n",
    "    for result in results:\n",
    "        model_name = result['Model']\n",
    "        model = result['Best Model']\n",
    "\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "        elif hasattr(model, 'decision_function'):\n",
    "            y_pred_proba = model.decision_function(X_test_scaled)\n",
    "        else:\n",
    "            raise AttributeError(\"Model has neither predict_proba nor decision_function\")\n",
    "              \n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.2f})', linewidth=2.5)\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random (AUC = 0.50)')\n",
    "    \n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=12)\n",
    "    plt.ylabel('True Positive Rate', fontsize=12)\n",
    "    plt.title('ROC Curve for Female Data Across Selected IMFs in balanced data', fontsize=12)\n",
    "    plt.legend(loc=\"lower right\", fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nProcesando resultados para los IMFs seleccionados...\")\n",
    "print(\"Resultados de GridSearchCV:\")\n",
    "params = results_df['Parameters'].apply(lambda x: {k.replace('model__', ''): v for k, v in x.items()})\n",
    "params_df = pd.DataFrame(params.tolist())\n",
    "print(params_df)\n",
    "print()\n",
    "\n",
    "plot_roc_curves(results, X_test_scaled, y_test)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5852cf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
